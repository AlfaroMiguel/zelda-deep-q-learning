{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "political-auction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import progressbar\n",
    "\n",
    "import gym\n",
    "import gym_gvgai\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "# Constants\n",
    "\n",
    "IMG_H = 9\n",
    "IMG_W = 13\n",
    "STATE_SIZE = (IMG_H, IMG_W)\n",
    "\n",
    "# Tensorboard extension\n",
    "\n",
    "%load_ext tensorboard\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, enviroment):\n",
    "        \n",
    "        # Initialize atributes\n",
    "        self._state_size = STATE_SIZE\n",
    "        #self._state_size = enviroment.observation_space.n\n",
    "        self._action_size = enviroment.action_space.n\n",
    "        \n",
    "        self.expirience_replay = deque(maxlen=2000)\n",
    "        \n",
    "        # Initialize discount and exploration rate\n",
    "        self.gamma = 0.6\n",
    "        self.epsilon = 0.1\n",
    "        \n",
    "        # Build networks\n",
    "        self.q_network = self._build_compile_model()\n",
    "        self.target_network = self._build_compile_model()\n",
    "        self.align_target_model()\n",
    "\n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def _build_compile_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=STATE_SIZE))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self._action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.1))\n",
    "        return model\n",
    "\n",
    "    def align_target_model(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return enviroment.action_space.sample()\n",
    "        \n",
    "        tensor_state = tf.convert_to_tensor([state])\n",
    "        \n",
    "        q_values = self.q_network.predict(tensor_state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def retrain(self, batch_size):\n",
    "        minibatch = random.sample(self.expirience_replay, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, terminated in minibatch:\n",
    "            \n",
    "            tensor_state = tf.convert_to_tensor([state])\n",
    "            tensor_next_state = tf.convert_to_tensor([next_state])\n",
    "            \n",
    "            q_values = self.q_network.predict(tensor_state)\n",
    "            \n",
    "            if terminated:\n",
    "                q_values[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_network.predict(tensor_next_state)\n",
    "                q_values[0][action] = reward + self.gamma * np.amax(t)\n",
    "            \n",
    "            self.q_network.fit(tensor_state, q_values, epochs=1, verbose=0, callbacks=[tensorboard_callback])\n",
    "            \n",
    "    def save_model(self, model_name=\"q_network\"):\n",
    "        self.q_network.save(model_name)\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.q_network = keras.models.load_model(model_path)\n",
    "        self.align_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "diagnostic-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def grayToArray(array):\n",
    "    result = np.zeros((IMG_H, IMG_W))\n",
    "    for i in range(int(array.shape[0]/10)):\n",
    "        for j in range(int(array.shape[1]/10)):\n",
    "            result[i][j] = int(array[10*i+5, 10*j+5])\n",
    "    return result\n",
    "\n",
    "\n",
    "def grayConversion(image):\n",
    "    b = image[..., 0]\n",
    "    g = image[..., 1]\n",
    "    r = image[..., 2]\n",
    "    return 0.21 * r + 0.72 * g + 0.07 * b\n",
    "\n",
    "def reshape_state(state):\n",
    "    return grayToArray(grayConversion(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "julian-anger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to host 127.0.0.1 at port 55519 ...\n",
      "Client connected to server [OK]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 117)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                2832      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 150       \n",
      "=================================================================\n",
      "Total params: 3,582\n",
      "Trainable params: 3,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "batch_size = 32\n",
    "num_of_episodes = 100\n",
    "timesteps_per_episode = 1000\n",
    "\n",
    "enviroment = gym.make(\"gvgai-zelda-lvl0-v0\")\n",
    "agent = Agent(enviroment)\n",
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-chest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[========================================================================] 100%\n",
      "[========================================================================] 100%\n",
      "[========================================================================] 100%\n",
      "[=====                                                                   ]   7%\r"
     ]
    }
   ],
   "source": [
    "for e in range(0, num_of_episodes):\n",
    "    # Reset the enviroment\n",
    "    state = enviroment.reset()\n",
    "    state = reshape_state(state)\n",
    "    \n",
    "    # Initialize variables\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=timesteps_per_episode/10, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    for timestep in range(timesteps_per_episode):\n",
    "        # Run Action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Take action    \n",
    "        next_state, reward, terminated, info = enviroment.step(action) \n",
    "        next_state = reshape_state(next_state)\n",
    "        agent.store(state, action, reward, next_state, terminated)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminated:\n",
    "            agent.align_target_model()\n",
    "            break\n",
    "            \n",
    "        if len(agent.expirience_replay) > batch_size:\n",
    "            agent.retrain(batch_size)\n",
    "        \n",
    "        if timestep%10 == 0:\n",
    "            bar.update(timestep/10 + 1)\n",
    "    \n",
    "    bar.finish()\n",
    "    if (e + 1) % 10 == 0:\n",
    "        print(\"**********************************\")\n",
    "        print(\"Episode: {}\".format(e + 1))\n",
    "        enviroment.render()\n",
    "        print(\"**********************************\")\n",
    "        agent.save_model(\"zelda_2_q_network_{}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "potential-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to host 127.0.0.1 at port 52304 ...\n",
      "Client connected to server [OK]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: zelda_q_network_1/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-33ee044065fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menviroment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zelda_q_network_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menviroment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-b0cbc58706d9>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_target_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: zelda_q_network_1/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# Play\n",
    "\n",
    "enviroment = gym.make(\"gvgai-zelda-lvl0-v0\")\n",
    "agent = Agent(enviroment)\n",
    "\n",
    "agent.load_model(\"zelda_q_network_1\")\n",
    "\n",
    "state = enviroment.reset()\n",
    "state = reshape_state(state)\n",
    "\n",
    "for timestep in range(2000):\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, terminated, info = enviroment.step(action)\n",
    "    print(action, reward, terminated, info)\n",
    "    state = reshape_state(next_state)\n",
    "    if terminated:\n",
    "        print(\"terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-terror",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-burns",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
