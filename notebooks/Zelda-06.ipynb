{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "political-auction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "TensorFlow version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import progressbar\n",
    "\n",
    "import gym\n",
    "import gym_gvgai\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "# Constants\n",
    "\n",
    "IMG_H = 9\n",
    "IMG_W = 13\n",
    "STATE_SIZE = (IMG_H, IMG_W)\n",
    "\n",
    "PLAYER_TILE_VALUE = 201\n",
    "SECONDARY_PLAYER_TILE_VALUE = 38\n",
    "\n",
    "# Tensorboard extension\n",
    "\n",
    "%load_ext tensorboard\n",
    "logdir = \"logs/scalars/zelda-06\"\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, enviroment):\n",
    "        \n",
    "        # Initialize atributes\n",
    "        self._state_size = STATE_SIZE\n",
    "        #self._state_size = enviroment.observation_space.n\n",
    "        self._action_size = enviroment.action_space.n\n",
    "        \n",
    "        self.expirience_replay = deque(maxlen=2000)\n",
    "        \n",
    "        # Initialize discount and exploration rate\n",
    "        self.gamma = 0.6\n",
    "        self.epsilon = 1.0\n",
    "        self.dec_epsilon_rate = 0.0001\n",
    "        self.min_epsilon = 0.1\n",
    "        \n",
    "        # Build networks\n",
    "        self.q_network = self._build_compile_model()\n",
    "        self.target_network = self._build_compile_model()\n",
    "        self.align_target_model()\n",
    "\n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def _build_compile_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=STATE_SIZE))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self._action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.1))\n",
    "        return model\n",
    "\n",
    "    def align_target_model(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            self.epsilon = self.epsilon - self.dec_epsilon_rate if self.epsilon > self.min_epsilon else self.min_epsilon\n",
    "            return enviroment.action_space.sample()\n",
    "        \n",
    "        tensor_state = tf.convert_to_tensor([state])\n",
    "        \n",
    "        q_values = self.q_network.predict(tensor_state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def retrain(self, batch_size):\n",
    "        minibatch = random.sample(self.expirience_replay, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, terminated in minibatch:\n",
    "            \n",
    "            tensor_state = tf.convert_to_tensor([state])\n",
    "            tensor_next_state = tf.convert_to_tensor([next_state])\n",
    "            \n",
    "            q_values = self.q_network.predict(tensor_state)\n",
    "            \n",
    "            if terminated:\n",
    "                q_values[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_network.predict(tensor_next_state)\n",
    "                q_values[0][action] = reward + self.gamma * np.amax(t)\n",
    "            \n",
    "            self.q_network.fit(tensor_state, q_values, epochs=1, verbose=0, callbacks=[tensorboard_callback])\n",
    "            \n",
    "    def save_model(self, model_name=\"q_network\"):\n",
    "        self.q_network.save(model_name)\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.q_network = keras.models.load_model(model_path)\n",
    "        self.align_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "diagnostic-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def grayToArray(array):\n",
    "    result = np.zeros((IMG_H, IMG_W))\n",
    "    for i in range(int(array.shape[0]/10)):\n",
    "        for j in range(int(array.shape[1]/10)):\n",
    "            result[i][j] = int(array[10*i+5, 10*j+5])\n",
    "    return result\n",
    "\n",
    "\n",
    "def grayConversion(image):\n",
    "    b = image[..., 0]\n",
    "    g = image[..., 1]\n",
    "    r = image[..., 2]\n",
    "    return 0.21 * r + 0.72 * g + 0.07 * b\n",
    "\n",
    "def reshape_state(state):\n",
    "    return grayToArray(grayConversion(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "julian-anger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to host 127.0.0.1 at port 62470 ...\n",
      "Client connected to server [OK]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 117)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24)                2832      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 150       \n",
      "=================================================================\n",
      "Total params: 3,582\n",
      "Trainable params: 3,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "batch_size = 64\n",
    "num_of_episodes = 100\n",
    "timesteps_per_episode = 1000\n",
    "\n",
    "enviroment = gym.make(\"gvgai-zelda-lvl0-v0\")\n",
    "agent = Agent(enviroment)\n",
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-chest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: zelda_6_q_network_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   1%\r"
     ]
    }
   ],
   "source": [
    "def find_position(state: list, previous_position):\n",
    "        player_row_index = None\n",
    "        player_col_index = None\n",
    "\n",
    "        if previous_position is None or previous_position[0] is None:\n",
    "            for row_index, row in enumerate(state):\n",
    "                for col_index, value in enumerate(row):\n",
    "                    if value == PLAYER_TILE_VALUE or value == SECONDARY_PLAYER_TILE_VALUE:\n",
    "                        player_row_index = row_index\n",
    "                        player_col_index = col_index\n",
    "                        break\n",
    "                if not (player_row_index is None):\n",
    "                    break\n",
    "            return player_row_index, player_col_index\n",
    "\n",
    "        for row_offset in range(-2, 2, 1):\n",
    "            for col_offset in range(-2, 2, 1):\n",
    "                row_index = previous_position[0] + row_offset\n",
    "                col_index = previous_position[1] + col_offset\n",
    "                value = int(state[row_index][col_index])\n",
    "\n",
    "                # print('row_index, col_index, value', row_index, col_index, value)\n",
    "                if value == PLAYER_TILE_VALUE or value == SECONDARY_PLAYER_TILE_VALUE:\n",
    "                    player_row_index = row_index\n",
    "                    player_col_index = col_index\n",
    "                    break\n",
    "            if not (player_row_index is None):\n",
    "                break\n",
    "\n",
    "        return player_row_index, player_col_index\n",
    "\n",
    "def process_reward(current_state, next_state, action_id, raw_reward, is_over, info, position, next_position):\n",
    "        actions_list = ['ACTION_NIL', 'ACTION_USE', 'ACTION_LEFT',\n",
    "                        'ACTION_RIGHT', 'ACTION_DOWN', 'ACTION_UP']\n",
    "        action = actions_list[action_id]\n",
    "\n",
    "        is_winner = info[\"winner\"] != \"PLAYER_LOSES\" and info[\"winner\"] != \"NO_WINNER\"\n",
    "\n",
    "        if is_over and is_winner:\n",
    "            return 400\n",
    "\n",
    "        if is_over and not is_winner:\n",
    "            return -100\n",
    "\n",
    "        if raw_reward > 0:\n",
    "            return raw_reward * 40\n",
    "\n",
    "        if action in ['ACTION_NIL']:\n",
    "            return -2000\n",
    "\n",
    "        if action in ['ACTION_USE']:\n",
    "            return -1000\n",
    "\n",
    "        has_moved = next_position[0] != position[0] or next_position[1] != position[1]\n",
    "\n",
    "        if has_moved:\n",
    "            return 100\n",
    "\n",
    "        return -1\n",
    "    \n",
    "\n",
    "\n",
    "for e in range(0, num_of_episodes):\n",
    "    # Reset the enviroment\n",
    "    state = enviroment.reset()\n",
    "    state = reshape_state(state)\n",
    "    position = find_position(state, None)\n",
    "    \n",
    "    # Initialize variables\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=timesteps_per_episode/10, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    for timestep in range(timesteps_per_episode):\n",
    "        # Run Action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Take action    \n",
    "        next_state, reward, terminated, info = enviroment.step(action) \n",
    "        next_state = reshape_state(next_state)\n",
    "        next_position = find_position(next_state, position)\n",
    "        reward = process_reward(state, next_state, action, reward, terminated, info, position, next_position)\n",
    "        agent.store(state, action, reward, next_state, terminated)\n",
    "        \n",
    "        state = next_state\n",
    "        position = next_position\n",
    "        \n",
    "        if terminated:\n",
    "            agent.align_target_model()\n",
    "            break\n",
    "            \n",
    "        if len(agent.expirience_replay) > batch_size:\n",
    "            agent.retrain(batch_size)\n",
    "        \n",
    "        if timestep%10 == 0:\n",
    "            agent.align_target_model()\n",
    "            bar.update(timestep/10 + 1)\n",
    "    \n",
    "    bar.finish()\n",
    "    if (e + 1) % 10 == 0:\n",
    "        print(\"**********************************\")\n",
    "        print(\"Episode: {}\".format(e + 1))\n",
    "        enviroment.render()\n",
    "        print(\"**********************************\")\n",
    "    agent.save_model(\"zelda_6_q_network_{}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "potential-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to host 127.0.0.1 at port 52304 ...\n",
      "Client connected to server [OK]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: zelda_q_network_1/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-33ee044065fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menviroment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zelda_q_network_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menviroment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-b0cbc58706d9>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_target_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: zelda_q_network_1/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# Play\n",
    "\n",
    "enviroment = gym.make(\"gvgai-zelda-lvl0-v0\")\n",
    "agent = Agent(enviroment)\n",
    "\n",
    "agent.load_model(\"zelda_q_network_1\")\n",
    "\n",
    "state = enviroment.reset()\n",
    "state = reshape_state(state)\n",
    "\n",
    "for timestep in range(2000):\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, terminated, info = enviroment.step(action)\n",
    "    print(action, reward, terminated, info)\n",
    "    state = reshape_state(next_state)\n",
    "    if terminated:\n",
    "        print(\"terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-terror",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-burns",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
