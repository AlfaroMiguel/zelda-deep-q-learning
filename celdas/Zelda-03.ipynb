{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import progressbar\n",
    "\n",
    "import gym\n",
    "import gym_gvgai\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "# Constants\n",
    "\n",
    "IMG_H = 9\n",
    "IMG_W = 13\n",
    "STATE_SIZE = (IMG_H, IMG_W)\n",
    "\n",
    "PLAYER_TILE_VALUE = 201\n",
    "SECONDARY_PLAYER_TILE_VALUE = 38\n",
    "\n",
    "# Tensorboard extension\n",
    "\n",
    "%load_ext tensorboard\n",
    "logdir = \"logs/scalars/zelda-03\"\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, enviroment):\n",
    "        \n",
    "        # Initialize atributes\n",
    "        self._state_size = STATE_SIZE\n",
    "        #self._state_size = enviroment.observation_space.n\n",
    "        self._action_size = enviroment.action_space.n\n",
    "        \n",
    "        self.expirience_replay = deque(maxlen=2000)\n",
    "        \n",
    "        # Initialize discount and exploration rate\n",
    "        self.gamma = 0.6\n",
    "        self.epsilon = 1.0\n",
    "        self.dec_epsilon_rate = 0.0001\n",
    "        self.min_epsilon = 0.1\n",
    "        \n",
    "        # Build networks\n",
    "        self.q_network = self._build_compile_model()\n",
    "        self.target_network = self._build_compile_model()\n",
    "        self.align_target_model()\n",
    "\n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def _build_compile_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=STATE_SIZE))\n",
    "        model.add(Dense(96, activation='relu'))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dense(self._action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.01))\n",
    "        return model\n",
    "\n",
    "    def align_target_model(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def act(self, state, use_epsilon_strategy=True):\n",
    "        if np.random.rand() <= self.epsilon and use_epsilon_strategy:\n",
    "            self.epsilon = self.epsilon - self.dec_epsilon_rate if self.epsilon > self.min_epsilon else self.min_epsilon\n",
    "            return enviroment.action_space.sample()\n",
    "        \n",
    "        if not use_epsilon_strategy and np.random.rand() <= self.min_epsilon:\n",
    "            return enviroment.action_space.sample()\n",
    "            \n",
    "        tensor_state = tf.convert_to_tensor([state])\n",
    "        \n",
    "        q_values = self.q_network.predict(tensor_state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def retrain(self, batch_size):\n",
    "        minibatch = random.sample(self.expirience_replay, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, terminated in minibatch:\n",
    "            \n",
    "            tensor_state = tf.convert_to_tensor([state])\n",
    "            tensor_next_state = tf.convert_to_tensor([next_state])\n",
    "            \n",
    "            q_values = self.q_network.predict(tensor_state)\n",
    "            \n",
    "            if terminated:\n",
    "                q_values[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_network.predict(tensor_next_state)\n",
    "                q_values[0][action] = reward + self.gamma * np.amax(t)\n",
    "            \n",
    "            self.q_network.fit(tensor_state, q_values, epochs=1, verbose=0, callbacks=[tensorboard_callback])\n",
    "            \n",
    "    def save_model(self, model_name=\"models/zelda_03_q_network\"):\n",
    "        self.q_network.save(model_name)\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.q_network = keras.models.load_model(model_path)\n",
    "        self.align_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "def show_state(env, step=0, name=\"\", info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"{} | Step: {} {}\".format(name, step, info))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "def grayToArray(array):\n",
    "    result = np.zeros((IMG_H, IMG_W))\n",
    "    for i in range(int(array.shape[0]/10)):\n",
    "        for j in range(int(array.shape[1]/10)):\n",
    "            result[i][j] = int(array[10*i+5, 10*j+5])\n",
    "    return result\n",
    "\n",
    "\n",
    "def grayConversion(image):\n",
    "    b = image[..., 0]\n",
    "    g = image[..., 1]\n",
    "    r = image[..., 2]\n",
    "    return 0.21 * r + 0.72 * g + 0.07 * b\n",
    "\n",
    "def reshape_state(state):\n",
    "    return grayToArray(grayConversion(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "batch_size = 256\n",
    "num_of_episodes = 100\n",
    "timesteps_per_episode = 1000\n",
    "\n",
    "enviroment = gym.make(\"gvgai-zelda-lvl0-v0\")\n",
    "agent = Agent(enviroment)\n",
    "# agent.load_model(\"models/zelda_03_q_network\")\n",
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_position(state: list, previous_position):\n",
    "        player_row_index = None\n",
    "        player_col_index = None\n",
    "\n",
    "        if previous_position is None or previous_position[0] is None:\n",
    "            for row_index, row in enumerate(state):\n",
    "                for col_index, value in enumerate(row):\n",
    "                    if value == PLAYER_TILE_VALUE or value == SECONDARY_PLAYER_TILE_VALUE:\n",
    "                        player_row_index = row_index\n",
    "                        player_col_index = col_index\n",
    "                        break\n",
    "                if not (player_row_index is None):\n",
    "                    break\n",
    "            return player_row_index, player_col_index\n",
    "\n",
    "        for row_offset in range(-2, 2, 1):\n",
    "            for col_offset in range(-2, 2, 1):\n",
    "                row_index = previous_position[0] + row_offset\n",
    "                col_index = previous_position[1] + col_offset\n",
    "                value = int(state[row_index][col_index])\n",
    "\n",
    "                # print('row_index, col_index, value', row_index, col_index, value)\n",
    "                if value == PLAYER_TILE_VALUE or value == SECONDARY_PLAYER_TILE_VALUE:\n",
    "                    player_row_index = row_index\n",
    "                    player_col_index = col_index\n",
    "                    break\n",
    "            if not (player_row_index is None):\n",
    "                break\n",
    "\n",
    "        return player_row_index, player_col_index\n",
    "\n",
    "def process_reward(current_state, next_state, action_id, raw_reward, is_over, info, position, next_position):\n",
    "        actions_list = ['ACTION_NIL', 'ACTION_USE', 'ACTION_LEFT',\n",
    "                        'ACTION_RIGHT', 'ACTION_DOWN', 'ACTION_UP']\n",
    "        action = actions_list[action_id]\n",
    "\n",
    "        is_winner = info[\"winner\"] != \"PLAYER_LOSES\" and info[\"winner\"] != \"NO_WINNER\"\n",
    "\n",
    "        if is_over and is_winner:\n",
    "            return 100\n",
    "\n",
    "        if is_over and not is_winner:\n",
    "            return -100\n",
    "\n",
    "        if raw_reward > 0:\n",
    "            return raw_reward * 10\n",
    "\n",
    "        if action in ['ACTION_NIL']:\n",
    "            return -200\n",
    "\n",
    "        if action in ['ACTION_USE']:\n",
    "            return -100\n",
    "\n",
    "        has_moved = next_position[0] != position[0] or next_position[1] != position[1]\n",
    "\n",
    "        if not has_moved:\n",
    "            return -5\n",
    "\n",
    "        return -1\n",
    "\n",
    "for e in range(0, num_of_episodes):\n",
    "    # Reset the enviroment\n",
    "    state = enviroment.reset()\n",
    "    state = reshape_state(state)\n",
    "    position = find_position(state, None)\n",
    "    \n",
    "    # Initialize variables\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=timesteps_per_episode/10, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    for timestep in range(timesteps_per_episode):\n",
    "        # Run Action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Take action    \n",
    "        next_state, reward, terminated, info = enviroment.step(action) \n",
    "        next_state = reshape_state(next_state)\n",
    "        next_position = find_position(next_state, position)\n",
    "        reward = process_reward(state, next_state, action, reward, terminated, info, position, next_position)\n",
    "        agent.store(state, action, reward, next_state, terminated)\n",
    "        \n",
    "        state = next_state\n",
    "        position = next_position\n",
    "        \n",
    "        if terminated:\n",
    "            agent.align_target_model()\n",
    "            break\n",
    "            \n",
    "        if len(agent.expirience_replay) > batch_size:\n",
    "            agent.retrain(batch_size)\n",
    "        \n",
    "        if timestep%10 == 0:\n",
    "            agent.align_target_model()\n",
    "            bar.update(timestep/10 + 1)\n",
    "    \n",
    "    bar.finish()\n",
    "    if (e + 1) % 10 == 0:\n",
    "        print(\"**********************************\")\n",
    "        print(\"Episode: {}\".format(e + 1))\n",
    "        print(\"**********************************\")\n",
    "    agent.save_model(\"models/zelda_03_q_network_{}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with agent\n",
    "enviroment = gym.make(\"gvgai-zelda-lvl0-v0\")\n",
    "actions_list = ['ACTION_NIL', 'ACTION_USE', 'ACTION_LEFT',\n",
    "                        'ACTION_RIGHT', 'ACTION_DOWN', 'ACTION_UP']\n",
    "agent = Agent(enviroment)\n",
    "\n",
    "model_path = \"models/zelda_3_q_network\"\n",
    "timesteps = 2000\n",
    "\n",
    "agent.load_model(model_path)\n",
    "state = reshape_state(enviroment.reset())\n",
    "\n",
    "for timestep in range(timesteps):\n",
    "    action = agent.act(state, False)\n",
    "    \n",
    "    next_state, reward, terminated, info = enviroment.step(action)\n",
    "    state = reshape_state(next_state)\n",
    "    \n",
    "    print(action, reward, terminated, info)\n",
    "    show_state(enviroment, timestep, \"Zelda\", \"Action: {} Player Status: {} Terminated: {}\".format(actions_list[action], info['winner'], terminated))\n",
    "\n",
    "    if terminated:\n",
    "        print(\"terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-summary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
