{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import progressbar\n",
    "\n",
    "import gym\n",
    "import gym_gvgai\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "# Constants\n",
    "\n",
    "IMG_H = 9\n",
    "IMG_W = 13\n",
    "STATE_SIZE = (IMG_H, IMG_W)\n",
    "\n",
    "# Tensorboard extension\n",
    "\n",
    "%load_ext tensorboard\n",
    "logdir = \"logs/scalars/zelda-02\"\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, enviroment):\n",
    "        \n",
    "        # Initialize atributes\n",
    "        self._state_size = STATE_SIZE\n",
    "        #self._state_size = enviroment.observation_space.n\n",
    "        self._action_size = enviroment.action_space.n\n",
    "        \n",
    "        self.expirience_replay = deque(maxlen=2000)\n",
    "        \n",
    "        # Initialize discount and exploration rate\n",
    "        self.gamma = 0.6\n",
    "        self.epsilon = 1.0\n",
    "        self.dec_epsilon_rate = 0.0001\n",
    "        self.min_epsilon = 0.1\n",
    "        \n",
    "        # Build networks\n",
    "        self.q_network = self._build_compile_model()\n",
    "        self.target_network = self._build_compile_model()\n",
    "        self.align_target_model()\n",
    "\n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def _build_compile_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=STATE_SIZE))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self._action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.1))\n",
    "        return model\n",
    "\n",
    "    def align_target_model(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            self.epsilon = self.epsilon - self.dec_epsilon_rate if self.epsilon > self.min_epsilon else self.min_epsilon\n",
    "            return enviroment.action_space.sample()\n",
    "        \n",
    "        tensor_state = tf.convert_to_tensor([state])\n",
    "        \n",
    "        q_values = self.q_network.predict(tensor_state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def retrain(self, batch_size):\n",
    "        minibatch = random.sample(self.expirience_replay, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, terminated in minibatch:\n",
    "            \n",
    "            tensor_state = tf.convert_to_tensor([state])\n",
    "            tensor_next_state = tf.convert_to_tensor([next_state])\n",
    "            \n",
    "            q_values = self.q_network.predict(tensor_state)\n",
    "            \n",
    "            if terminated:\n",
    "                q_values[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_network.predict(tensor_next_state)\n",
    "                q_values[0][action] = reward + self.gamma * np.amax(t)\n",
    "            \n",
    "            self.q_network.fit(tensor_state, q_values, epochs=1, verbose=0, callbacks=[tensorboard_callback])\n",
    "            \n",
    "    def save_model(self, model_name=\"models/zelda_02_q_network\"):\n",
    "        self.q_network.save(model_name)\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.q_network = keras.models.load_model(model_path)\n",
    "        self.align_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "def show_state(env, step=0, name=\"\", info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"{} | Step: {} {}\".format(name, step, info))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "def grayToArray(array):\n",
    "    result = np.zeros((IMG_H, IMG_W))\n",
    "    for i in range(int(array.shape[0]/10)):\n",
    "        for j in range(int(array.shape[1]/10)):\n",
    "            result[i][j] = int(array[10*i+5, 10*j+5])\n",
    "    return result\n",
    "\n",
    "\n",
    "def grayConversion(image):\n",
    "    b = image[..., 0]\n",
    "    g = image[..., 1]\n",
    "    r = image[..., 2]\n",
    "    return 0.21 * r + 0.72 * g + 0.07 * b\n",
    "\n",
    "def reshape_state(state):\n",
    "    return grayToArray(grayConversion(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "batch_size = 32\n",
    "num_of_episodes = 100\n",
    "timesteps_per_episode = 1000\n",
    "\n",
    "enviroment = gym.make(\"gvgai-zelda-lvl0-v0\")\n",
    "agent = Agent(enviroment)\n",
    "# agent.load_model(\"models/zelda_02_q_network\")\n",
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reward(current_state, next_state, action_id, raw_reward, is_over, info):\n",
    "    actions_list = ['ACTION_NIL', 'ACTION_USE', 'ACTION_LEFT',\n",
    "                    'ACTION_RIGHT', 'ACTION_DOWN', 'ACTION_UP']\n",
    "    action = actions_list[action_id]\n",
    "\n",
    "    is_winner = info[\"winner\"] != \"PLAYER_LOSES\" and info[\"winner\"] != \"NO_WINNER\"\n",
    "\n",
    "    if is_over and is_winner:\n",
    "        return 100\n",
    "\n",
    "    if is_over and not is_winner:\n",
    "        return -100\n",
    "\n",
    "    if raw_reward > 0:\n",
    "        return raw_reward * 10\n",
    "\n",
    "    if action in ['ACTION_NIL', 'ACTION_USE']:\n",
    "        return -100\n",
    "    \n",
    "    if raw_reward == 0:\n",
    "        return -10\n",
    "    \n",
    "\n",
    "\n",
    "for e in range(0, num_of_episodes):\n",
    "    # Reset the enviroment\n",
    "    state = enviroment.reset()\n",
    "    state = reshape_state(state)\n",
    "    \n",
    "    # Initialize variables\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=timesteps_per_episode/10, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    for timestep in range(timesteps_per_episode):\n",
    "        # Run Action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Take action    \n",
    "        next_state, reward, terminated, info = enviroment.step(action) \n",
    "        next_state = reshape_state(next_state)\n",
    "        reward = process_reward(state, next_state, action, reward, terminated, info)\n",
    "        agent.store(state, action, reward, next_state, terminated)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminated:\n",
    "            agent.align_target_model()\n",
    "            break\n",
    "            \n",
    "        if len(agent.expirience_replay) > batch_size:\n",
    "            agent.retrain(batch_size)\n",
    "        \n",
    "        if timestep%10 == 0:\n",
    "            agent.align_target_model()\n",
    "            bar.update(timestep/10 + 1)\n",
    "    \n",
    "    bar.finish()\n",
    "    if (e + 1) % 10 == 0:\n",
    "        print(\"**********************************\")\n",
    "        print(\"Episode: {}\".format(e + 1))\n",
    "        print(\"**********************************\")\n",
    "        agent.save_model(\"models/zelda_02_q_network_{}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with agent\n",
    "enviroment = gym.make(\"gvgai-zelda-lvl0-v0\")\n",
    "agent = Agent(enviroment)\n",
    "\n",
    "model_path = \"models/zelda_02_q_network\"\n",
    "timesteps = 2000\n",
    "\n",
    "agent.load_model(model_path)\n",
    "state = reshape_state(enviroment.reset())\n",
    "\n",
    "for timestep in range(timesteps):\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    next_state, reward, terminated, info = enviroment.step(action)\n",
    "    state = reshape_state(next_state)\n",
    "    \n",
    "    print(action, reward, terminated, info)\n",
    "    show_state(enviroment, timestep, \"Zelda\", \"Action: {} Player Status: {} Terminated: {}\".format(info['actions'][action], info['winner'], terminated))\n",
    "\n",
    "    if terminated:\n",
    "        print(\"terminated\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
